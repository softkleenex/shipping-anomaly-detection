"""
ìµœì¢… ëª¨ë¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•œ ìµœì í™”ëœ ë²„ì „
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import VotingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

from datetime import datetime
import sys
import os

# í˜„ìž¬ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils import load_data, get_feature_columns, save_submission, print_target_distribution

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)


def create_quick_features(df):
    """ë¹ ë¥¸ Feature Engineering (í•µì‹¬ í”¼ì²˜ë§Œ)"""
    df_new = df.copy()

    # í•µì‹¬ í†µê³„ íŠ¹ì§•ë§Œ
    df_new['row_mean'] = df.mean(axis=1)
    df_new['row_std'] = df.std(axis=1)
    df_new['row_max'] = df.max(axis=1)
    df_new['row_min'] = df.min(axis=1)
    df_new['row_range'] = df_new['row_max'] - df_new['row_min']

    # ë¹„ìœ¨ íŠ¹ì§•
    df_new['mean_to_std_ratio'] = df_new['row_mean'] / (df_new['row_std'] + 1e-10)

    # ì¹´ìš´íŠ¸ íŠ¹ì§•
    df_new['positive_count'] = (df > 0).sum(axis=1)
    df_new['negative_count'] = (df < 0).sum(axis=1)

    return df_new


def get_final_ensemble():
    """ê²€ì¦ëœ ìµœì¢… ì•™ìƒë¸” ëª¨ë¸"""

    # ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    lgb = LGBMClassifier(
        n_estimators=600,
        max_depth=10,
        learning_rate=0.05,
        num_leaves=50,
        min_child_samples=20,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.1,
        reg_lambda=0.1,
        random_state=RANDOM_STATE,
        n_jobs=-1,
        verbose=-1,
        objective='multiclass',
        class_weight='balanced'
    )

    xgb = XGBClassifier(
        n_estimators=600,
        max_depth=10,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.1,
        reg_lambda=0.1,
        gamma=0.1,
        min_child_weight=3,
        random_state=RANDOM_STATE,
        n_jobs=-1,
        objective='multi:softprob',
        use_label_encoder=False
    )

    cat = CatBoostClassifier(
        iterations=800,
        depth=10,
        learning_rate=0.05,
        random_state=RANDOM_STATE,
        verbose=False,
        auto_class_weights='Balanced',
        l2_leaf_reg=5
    )

    # Voting Ensemble
    ensemble = VotingClassifier(
        estimators=[
            ('lgb', lgb),
            ('xgb', xgb),
            ('cat', cat)
        ],
        voting='soft',
        n_jobs=-1
    )

    return ensemble


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    print("="*70)
    print("  ðŸš€ ì´ìƒì‹ í˜¸ ê°ì§€ ê¸°ë°˜ ë¹„ì •ìƒ ìž‘ë™ ì§„ë‹¨ - ìµœì¢… ëª¨ë¸")
    print("  ëª©í‘œ: Macro-F1 Score > 0.9")
    print("="*70)

    # 1. ë°ì´í„° ë¡œë“œ
    print("\n[STEP 1] ë°ì´í„° ë¡œë“œ...")
    train, test, submission = load_data()

    # í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬
    feature_cols = get_feature_columns(train)
    X_train = train[feature_cols]
    y_train = train['target']
    X_test = test[feature_cols]

    print(f"  âœ“ Train: {X_train.shape}")
    print(f"  âœ“ Test: {X_test.shape}")

    # íƒ€ê²Ÿ ë¶„í¬ í™•ì¸
    print_target_distribution(y_train, "Train Target Distribution")

    # 2. Feature Engineering
    print("\n[STEP 2] Feature Engineering...")
    X_train = create_quick_features(X_train)
    X_test = create_quick_features(X_test)
    print(f"  âœ“ Features created: {X_train.shape[1]}")

    # 3. ìŠ¤ì¼€ì¼ë§
    print("\n[STEP 3] ë°ì´í„° ìŠ¤ì¼€ì¼ë§...")
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    print("  âœ“ Scaling completed")

    # 4. ëª¨ë¸ í•™ìŠµ
    print("\n[STEP 4] ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ...")
    ensemble = get_final_ensemble()

    print("  ëª¨ë¸ í•™ìŠµ ì¤‘... (ì•½ 2-3ë¶„ ì†Œìš”)")
    ensemble.fit(X_train_scaled, y_train)
    print("  âœ“ Training completed")

    # 5. ì˜ˆì¸¡
    print("\n[STEP 5] í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡...")
    predictions = ensemble.predict(X_test_scaled)
    print("  âœ“ Predictions completed")

    # ì˜ˆì¸¡ ë¶„í¬ í™•ì¸
    print_target_distribution(predictions, "Prediction Distribution")

    # 6. ì œì¶œ íŒŒì¼ ìƒì„±
    print("\n[STEP 6] ì œì¶œ íŒŒì¼ ìƒì„±...")
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    submission_path = f'../submissions/final_ensemble_{timestamp}.csv'
    save_submission(predictions, submission, submission_path)

    print("\n" + "="*70)
    print("  âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print("  ì œì¶œ íŒŒì¼: " + submission_path)
    print("="*70)

    return predictions


if __name__ == "__main__":
    predictions = main()

    # ì¶”ê°€ ì •ë³´ ì¶œë ¥
    print("\n[ì¶”ê°€ ì •ë³´]")
    print("- ëª¨ë¸: LightGBM + XGBoost + CatBoost Voting Ensemble")
    print("- Feature Engineering: í†µê³„ì  íŠ¹ì§• + ë¹„ìœ¨ íŠ¹ì§•")
    print("- Scaling: RobustScaler (ì´ìƒì¹˜ì— ê°•í•¨)")
    print("- Class Weight: Balanced (í´ëž˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬)")
    print("\nì œì¶œ ì „ ë¦¬ë”ë³´ë“œë¥¼ í™•ì¸í•˜ê³  ì ìˆ˜ë¥¼ ê¸°ë¡í•˜ì„¸ìš”!")
    print("ëª©í‘œ: Macro-F1 Score > 0.9")