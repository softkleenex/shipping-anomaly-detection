{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì´ìƒì‹ í˜¸ ê°ì§€ ê¸°ë°˜ ë¹„ì •ìƒ ì‘ë™ ì§„ë‹¨ - EDA\n",
    "\n",
    "## ëª©í‘œ\n",
    "- Macro-F1 Score > 0.9 ë‹¬ì„±\n",
    "- ë°ì´í„° íŠ¹ì„± íŒŒì•… ë° ì „ì²˜ë¦¬ ì „ëµ ìˆ˜ë¦½\n",
    "\n",
    "## í‰ê°€ ì§€í‘œ\n",
    "- **Macro-F1 Score**: ê° í´ë˜ìŠ¤ì˜ F1 Scoreë¥¼ í‰ê· ë‚¸ ê°’\n",
    "- í´ë˜ìŠ¤ ë¶ˆê· í˜•ì— ë¯¼ê°í•˜ë¯€ë¡œ ëª¨ë“  í´ë˜ìŠ¤ë¥¼ ê· ë“±í•˜ê²Œ ì˜ ì˜ˆì¸¡í•´ì•¼ í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print('ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "train = pd.read_csv('../open/train.csv')\n",
    "test = pd.read_csv('../open/test.csv')\n",
    "submission = pd.read_csv('../open/sample_submission.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"\\ní”¼ì²˜ ìˆ˜: {train.shape[1] - 2} (ID, target ì œì™¸)\")\n",
    "print(f\"Train ìƒ˜í”Œ ìˆ˜: {train.shape[0]:,}\")\n",
    "print(f\"Test ìƒ˜í”Œ ìˆ˜: {test.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "print(\"Train ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):\")\n",
    "display(train.head())\n",
    "\n",
    "print(\"\\nì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸:\")\n",
    "print(train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° íƒ€ì… í™•ì¸\n",
    "print(\"ë°ì´í„° íƒ€ì…:\")\n",
    "print(train.dtypes.value_counts())\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\n",
    "print(f\"\\në©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:\")\n",
    "print(f\"Train: {train.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Test: {test.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target ë¶„í¬ ë¶„ì„ (ì¤‘ìš”!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target ë¶„í¬ í™•ì¸\n",
    "target_counts = train['target'].value_counts()\n",
    "target_ratio = train['target'].value_counts(normalize=True)\n",
    "\n",
    "print(\"Target í´ë˜ìŠ¤ ë¶„í¬:\")\n",
    "print(\"=\"*50)\n",
    "for cls in sorted(target_counts.index):\n",
    "    print(f\"Class {cls}: {target_counts[cls]:6d} ({target_ratio[cls]*100:5.2f}%)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ì´ í´ë˜ìŠ¤ ìˆ˜: {train['target'].nunique()}\")\n",
    "print(f\"\\ní´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨ (ìµœëŒ€/ìµœì†Œ): {target_counts.max() / target_counts.min():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target ë¶„í¬ ì‹œê°í™”\n",
    "fig = make_subplots(rows=1, cols=2, \n",
    "                    subplot_titles=['Target í´ë˜ìŠ¤ ë¶„í¬', 'Target í´ë˜ìŠ¤ ë¹„ìœ¨'],\n",
    "                    specs=[[{'type': 'bar'}, {'type': 'pie'}]])\n",
    "\n",
    "# ë§‰ëŒ€ ì°¨íŠ¸\n",
    "fig.add_trace(\n",
    "    go.Bar(x=target_counts.index, y=target_counts.values, \n",
    "           text=target_counts.values, textposition='auto',\n",
    "           name='Count'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# íŒŒì´ ì°¨íŠ¸\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=target_counts.index, values=target_counts.values,\n",
    "           textinfo='label+percent', hole=0.3),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"Target í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ê²°ì¸¡ì¹˜ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "train_nulls = train.isnull().sum()\n",
    "test_nulls = test.isnull().sum()\n",
    "\n",
    "print(\"Train ê²°ì¸¡ì¹˜:\")\n",
    "print(f\"ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼ ìˆ˜: {(train_nulls > 0).sum()}\")\n",
    "if train_nulls.sum() > 0:\n",
    "    print(train_nulls[train_nulls > 0])\n",
    "else:\n",
    "    print(\"ê²°ì¸¡ì¹˜ ì—†ìŒ\")\n",
    "\n",
    "print(\"\\nTest ê²°ì¸¡ì¹˜:\")\n",
    "print(f\"ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼ ìˆ˜: {(test_nulls > 0).sum()}\")\n",
    "if test_nulls.sum() > 0:\n",
    "    print(test_nulls[test_nulls > 0])\n",
    "else:\n",
    "    print(\"ê²°ì¸¡ì¹˜ ì—†ìŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í”¼ì²˜ ë¶„í¬ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”¼ì²˜ ê¸°ë³¸ í†µê³„\n",
    "feature_cols = [col for col in train.columns if col not in ['ID', 'target']]\n",
    "print(f\"ë¶„ì„í•  í”¼ì²˜ ìˆ˜: {len(feature_cols)}\")\n",
    "\n",
    "# ê¸°ìˆ  í†µê³„\n",
    "train_stats = train[feature_cols].describe()\n",
    "print(\"\\nê¸°ìˆ  í†µê³„ (ì¼ë¶€):\")\n",
    "display(train_stats.iloc[:, :10])  # ì²˜ìŒ 10ê°œ í”¼ì²˜ë§Œ í‘œì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê° í”¼ì²˜ì˜ ë¶„í¬ íŠ¹ì„± ë¶„ì„\n",
    "feature_info = pd.DataFrame()\n",
    "feature_info['feature'] = feature_cols\n",
    "feature_info['mean'] = train[feature_cols].mean().values\n",
    "feature_info['std'] = train[feature_cols].std().values\n",
    "feature_info['min'] = train[feature_cols].min().values\n",
    "feature_info['max'] = train[feature_cols].max().values\n",
    "feature_info['nunique'] = train[feature_cols].nunique().values\n",
    "feature_info['skew'] = train[feature_cols].skew().values\n",
    "feature_info['kurt'] = train[feature_cols].kurtosis().values\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ ì°¨ì´ ë¶„ì„\n",
    "feature_info['scale_ratio'] = feature_info['max'] - feature_info['min']\n",
    "\n",
    "print(\"í”¼ì²˜ë³„ íŠ¹ì„± ìš”ì•½:\")\n",
    "display(feature_info.head(10))\n",
    "\n",
    "print(f\"\\nìœ ë‹ˆí¬ ê°’ì´ ì ì€ í”¼ì²˜ (ì´ì§„ ë˜ëŠ” ë²”ì£¼í˜• ê°€ëŠ¥ì„±):\")\n",
    "low_unique = feature_info[feature_info['nunique'] <= 10]\n",
    "print(f\"ê°œìˆ˜: {len(low_unique)}\")\n",
    "if len(low_unique) > 0:\n",
    "    print(low_unique[['feature', 'nunique']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”¼ì²˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ (ìƒ˜í”Œ)\n",
    "sample_features = feature_cols[:12]  # ì²˜ìŒ 12ê°œ í”¼ì²˜ë§Œ\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(sample_features):\n",
    "    train[col].hist(bins=50, ax=axes[idx], edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{col}\\n(unique: {train[col].nunique()})', fontsize=10)\n",
    "    axes[idx].set_xlabel('')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('í”¼ì²˜ ë¶„í¬ ìƒ˜í”Œ (ì²˜ìŒ 12ê°œ)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. íƒ€ê²Ÿë³„ í”¼ì²˜ ë¶„í¬ ì°¨ì´ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íƒ€ê²Ÿë³„ í”¼ì²˜ í‰ê· ê°’ ì°¨ì´ ë¶„ì„\n",
    "target_means = train.groupby('target')[feature_cols].mean()\n",
    "\n",
    "# ê° í”¼ì²˜ë³„ë¡œ íƒ€ê²Ÿ ê°„ ì°¨ì´ê°€ í°ì§€ ë¶„ì„\n",
    "feature_importance = pd.DataFrame()\n",
    "feature_importance['feature'] = feature_cols\n",
    "\n",
    "# íƒ€ê²Ÿë³„ í‰ê· ì˜ í‘œì¤€í¸ì°¨ (íƒ€ê²Ÿ ê°„ ì°¨ì´ê°€ í´ìˆ˜ë¡ ë†’ìŒ)\n",
    "feature_importance['target_std'] = target_means.std().values\n",
    "\n",
    "# ì „ì²´ í‘œì¤€í¸ì°¨ ëŒ€ë¹„ íƒ€ê²Ÿ ê°„ í‘œì¤€í¸ì°¨ ë¹„ìœ¨\n",
    "overall_std = train[feature_cols].std()\n",
    "feature_importance['importance_ratio'] = feature_importance['target_std'] / overall_std.values\n",
    "\n",
    "# ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "feature_importance = feature_importance.sort_values('importance_ratio', ascending=False)\n",
    "\n",
    "print(\"íƒ€ê²Ÿ êµ¬ë¶„ë ¥ì´ ë†’ì€ ìƒìœ„ 15ê°œ í”¼ì²˜:\")\n",
    "display(feature_importance.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒìœ„ ì¤‘ìš” í”¼ì²˜ì˜ íƒ€ê²Ÿë³„ ë¶„í¬ ì‹œê°í™”\n",
    "top_features = feature_importance.head(6)['feature'].values\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(top_features):\n",
    "    for target_val in sorted(train['target'].unique()):\n",
    "        subset = train[train['target'] == target_val][col]\n",
    "        axes[idx].hist(subset, bins=30, alpha=0.5, label=f'Target {target_val}', density=True)\n",
    "    \n",
    "    axes[idx].set_title(f'{col}', fontsize=10)\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Density')\n",
    "    axes[idx].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('íƒ€ê²Ÿë³„ í”¼ì²˜ ë¶„í¬ (ìƒìœ„ 6ê°œ ì¤‘ìš” í”¼ì²˜)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ìƒê´€ê´€ê³„ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”¼ì²˜ ê°„ ìƒê´€ê´€ê³„\n",
    "correlation_matrix = train[feature_cols].corr()\n",
    "\n",
    "# ë†’ì€ ìƒê´€ê´€ê³„ ìŒ ì°¾ê¸°\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append({\n",
    "                'feature1': correlation_matrix.columns[i],\n",
    "                'feature2': correlation_matrix.columns[j],\n",
    "                'correlation': correlation_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('correlation', ascending=False)\n",
    "    print(f\"ë†’ì€ ìƒê´€ê´€ê³„ (|r| > 0.8) ìŒ ìˆ˜: {len(high_corr_df)}\")\n",
    "    print(\"\\nìƒìœ„ 10ê°œ:\")\n",
    "    display(high_corr_df.head(10))\n",
    "else:\n",
    "    print(\"ë†’ì€ ìƒê´€ê´€ê³„ (|r| > 0.8) ìŒì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íƒ€ê²Ÿê³¼ í”¼ì²˜ ê°„ ìƒê´€ê´€ê³„ (ì„ í˜• ê´€ê³„ë§Œ íŒŒì•… ê°€ëŠ¥)\n",
    "target_corr = train[feature_cols + ['target']].corr()['target'].drop('target')\n",
    "target_corr = target_corr.sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"íƒ€ê²Ÿê³¼ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ìƒìœ„ 15ê°œ í”¼ì²˜:\")\n",
    "print(target_corr.head(15))\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(10, 6))\n",
    "target_corr.head(20).plot(kind='barh')\n",
    "plt.xlabel('Correlation with Target')\n",
    "plt.title('Top 20 Features by Correlation with Target')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì´ìƒì¹˜ íƒì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€\n",
    "def count_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return len(outliers)\n",
    "\n",
    "outlier_counts = {}\n",
    "for col in feature_cols:\n",
    "    outlier_counts[col] = count_outliers_iqr(train, col)\n",
    "\n",
    "outlier_df = pd.DataFrame(list(outlier_counts.items()), columns=['feature', 'outlier_count'])\n",
    "outlier_df['outlier_ratio'] = outlier_df['outlier_count'] / len(train)\n",
    "outlier_df = outlier_df.sort_values('outlier_ratio', ascending=False)\n",
    "\n",
    "print(\"ì´ìƒì¹˜ê°€ ë§ì€ ìƒìœ„ 10ê°œ í”¼ì²˜:\")\n",
    "display(outlier_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train/Test ë¶„í¬ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainê³¼ Testì˜ í”¼ì²˜ ë¶„í¬ ì°¨ì´ í™•ì¸\n",
    "distribution_diff = pd.DataFrame()\n",
    "distribution_diff['feature'] = feature_cols\n",
    "distribution_diff['train_mean'] = train[feature_cols].mean().values\n",
    "distribution_diff['test_mean'] = test[feature_cols].mean().values\n",
    "distribution_diff['mean_diff'] = abs(distribution_diff['train_mean'] - distribution_diff['test_mean'])\n",
    "distribution_diff['train_std'] = train[feature_cols].std().values\n",
    "distribution_diff['test_std'] = test[feature_cols].std().values\n",
    "distribution_diff['std_diff'] = abs(distribution_diff['train_std'] - distribution_diff['test_std'])\n",
    "\n",
    "# ì •ê·œí™”ëœ ì°¨ì´\n",
    "distribution_diff['normalized_diff'] = distribution_diff['mean_diff'] / (distribution_diff['train_std'] + 1e-10)\n",
    "\n",
    "distribution_diff = distribution_diff.sort_values('normalized_diff', ascending=False)\n",
    "\n",
    "print(\"Train/Test ë¶„í¬ ì°¨ì´ê°€ í° ìƒìœ„ 10ê°œ í”¼ì²˜:\")\n",
    "display(distribution_diff.head(10))\n",
    "\n",
    "if distribution_diff['normalized_diff'].max() > 0.5:\n",
    "    print(\"\\nâš ï¸ ê²½ê³ : ì¼ë¶€ í”¼ì²˜ì—ì„œ Train/Test ë¶„í¬ ì°¨ì´ê°€ í½ë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(\"\\nâœ… Train/Test ë¶„í¬ê°€ ì „ë°˜ì ìœ¼ë¡œ ìœ ì‚¬í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ë° ì „ëµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶„ì„ ê²°ê³¼ ìš”ì•½\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š ë°ì´í„° ë¶„ì„ ìš”ì•½\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n1. ë°ì´í„° ê·œëª¨:\")\n",
    "print(f\"   - Train: {train.shape[0]:,} samples\")\n",
    "print(f\"   - Test: {test.shape[0]:,} samples\")\n",
    "print(f\"   - Features: {len(feature_cols)} (X_01 ~ X_52)\")\n",
    "print(f\"   - Target Classes: {train['target'].nunique()}\")\n",
    "\n",
    "print(f\"\\n2. Target ë¶„í¬:\")\n",
    "print(f\"   - í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨: {target_counts.max() / target_counts.min():.2f}\")\n",
    "if target_counts.max() / target_counts.min() > 3:\n",
    "    print(f\"   - âš ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ í•„ìš”\")\n",
    "\n",
    "print(f\"\\n3. ê²°ì¸¡ì¹˜:\")\n",
    "print(f\"   - Train: {train_nulls.sum()} ê°œ\")\n",
    "print(f\"   - Test: {test_nulls.sum()} ê°œ\")\n",
    "\n",
    "print(f\"\\n4. í”¼ì²˜ íŠ¹ì„±:\")\n",
    "print(f\"   - ë‚®ì€ unique ê°’ í”¼ì²˜: {len(low_unique)} ê°œ\")\n",
    "print(f\"   - ë†’ì€ ìƒê´€ê´€ê³„ ìŒ: {len(high_corr_pairs) if 'high_corr_pairs' in locals() else 0} ê°œ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ ëª¨ë¸ë§ ì „ëµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "1. ë°ì´í„° ì „ì²˜ë¦¬:\n",
    "   - StandardScalerë¡œ ìŠ¤ì¼€ì¼ë§ (í”¼ì²˜ ê°„ ìŠ¤ì¼€ì¼ ì°¨ì´ ì¡´ì¬)\n",
    "   - ì´ìƒì¹˜ ì²˜ë¦¬ ê³ ë ¤ (íŠ¹ì • í”¼ì²˜ì— ì´ìƒì¹˜ ë‹¤ìˆ˜)\n",
    "\n",
    "2. Feature Engineering:\n",
    "   - í”¼ì²˜ ê°„ ìƒí˜¸ì‘ìš© íŠ¹ì§• ìƒì„±\n",
    "   - í†µê³„ì  íŠ¹ì§• ì¶”ê°€ (mean, std, max-min ë“±)\n",
    "   - PCA ë˜ëŠ” ì°¨ì› ì¶•ì†Œ ê³ ë ¤\n",
    "\n",
    "3. ëª¨ë¸ ì„ íƒ:\n",
    "   - Tree ê¸°ë°˜ ëª¨ë¸ (XGBoost, LightGBM, CatBoost)\n",
    "   - ì•™ìƒë¸” ë°©ë²• ì ìš©\n",
    "   - Macro-F1 ìµœì í™”ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì¡°ì •\n",
    "\n",
    "4. ê²€ì¦ ì „ëµ:\n",
    "   - Stratified K-Fold (K=5)\n",
    "   - Macro-F1 Score ëª¨ë‹ˆí„°ë§\n",
    "   - ê° í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ê· ë“±í™”\n",
    "\n",
    "ëª©í‘œ: Macro-F1 Score > 0.9\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì €ì¥ (ì „ì²˜ë¦¬ìš©)\n",
    "train.to_csv('../data/train.csv', index=False)\n",
    "test.to_csv('../data/test.csv', index=False)\n",
    "submission.to_csv('../data/sample_submission.csv', index=False)\n",
    "\n",
    "print(\"âœ… ë°ì´í„°ë¥¼ data í´ë”ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"\\në‹¤ìŒ ë‹¨ê³„: Baseline ëª¨ë¸ êµ¬ì¶•\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}