{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Ensemble Model\n",
    "\n",
    "## ëª©í‘œ\n",
    "- Feature Engineeringìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ\n",
    "- ì•™ìƒë¸” ê¸°ë²•ìœ¼ë¡œ Macro-F1 > 0.9 ë‹¬ì„±\n",
    "- ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.metrics import f1_score, classification_report, make_scorer\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "\n",
    "# Models\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Hyperparameter tuning\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print('ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "# ê¸°ë³¸ í”¼ì²˜\n",
    "feature_cols = [col for col in train.columns if col not in ['ID', 'target']]\n",
    "X_train_orig = train[feature_cols]\n",
    "y_train = train['target']\n",
    "X_test_orig = test[feature_cols]\n",
    "\n",
    "print(f\"Original features: {len(feature_cols)}\")\n",
    "print(f\"Train shape: {X_train_orig.shape}\")\n",
    "print(f\"Test shape: {X_test_orig.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # 1. í†µê³„ì  íŠ¹ì§•\n",
    "    df_new['row_mean'] = df.mean(axis=1)\n",
    "    df_new['row_std'] = df.std(axis=1)\n",
    "    df_new['row_max'] = df.max(axis=1)\n",
    "    df_new['row_min'] = df.min(axis=1)\n",
    "    df_new['row_median'] = df.median(axis=1)\n",
    "    df_new['row_range'] = df_new['row_max'] - df_new['row_min']\n",
    "    df_new['row_skew'] = df.skew(axis=1)\n",
    "    df_new['row_kurt'] = df.kurtosis(axis=1)\n",
    "    \n",
    "    # 2. ë¹„ìœ¨ íŠ¹ì§•\n",
    "    df_new['mean_to_std_ratio'] = df_new['row_mean'] / (df_new['row_std'] + 1e-10)\n",
    "    df_new['max_to_min_ratio'] = df_new['row_max'] / (df_new['row_min'] + 1e-10)\n",
    "    \n",
    "    # 3. ìƒìœ„ í”¼ì²˜ ê°„ ìƒí˜¸ì‘ìš© (ì¤‘ìš”ë„ê°€ ë†’ì€ í”¼ì²˜ë“¤)\n",
    "    important_features = ['X_01', 'X_02', 'X_03', 'X_04', 'X_05']  # ì˜ˆì‹œ\n",
    "    for i, feat1 in enumerate(important_features):\n",
    "        if feat1 in df.columns:\n",
    "            for feat2 in important_features[i+1:]:\n",
    "                if feat2 in df.columns:\n",
    "                    # ê³±ì…ˆ\n",
    "                    df_new[f'{feat1}_mul_{feat2}'] = df[feat1] * df[feat2]\n",
    "                    # ì°¨ì´\n",
    "                    df_new[f'{feat1}_diff_{feat2}'] = df[feat1] - df[feat2]\n",
    "                    # ë¹„ìœ¨\n",
    "                    df_new[f'{feat1}_ratio_{feat2}'] = df[feat1] / (df[feat2] + 1e-10)\n",
    "    \n",
    "    # 4. ë¶„ìœ„ìˆ˜ íŠ¹ì§•\n",
    "    df_new['row_q25'] = df.quantile(0.25, axis=1)\n",
    "    df_new['row_q75'] = df.quantile(0.75, axis=1)\n",
    "    df_new['row_iqr'] = df_new['row_q75'] - df_new['row_q25']\n",
    "    \n",
    "    # 5. ì¹´ìš´íŠ¸ íŠ¹ì§•\n",
    "    df_new['positive_count'] = (df > 0).sum(axis=1)\n",
    "    df_new['negative_count'] = (df < 0).sum(axis=1)\n",
    "    df_new['zero_count'] = (df == 0).sum(axis=1)\n",
    "    \n",
    "    # 6. ê·¹ê°’ íŠ¹ì§•\n",
    "    df_new['abs_max'] = df.abs().max(axis=1)\n",
    "    df_new['abs_min'] = df.abs().min(axis=1)\n",
    "    df_new['abs_mean'] = df.abs().mean(axis=1)\n",
    "    \n",
    "    print(f\"Features created: {len(df_new.columns) - len(df.columns)}\")\n",
    "    print(f\"Total features: {len(df_new.columns)}\")\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# í”¼ì²˜ ìƒì„±\n",
    "X_train_fe = create_features(X_train_orig)\n",
    "X_test_fe = create_features(X_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA íŠ¹ì§• ì¶”ê°€\n",
    "n_components = 20\n",
    "pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "\n",
    "# ì›ë³¸ í”¼ì²˜ì— ëŒ€í•´ PCA ì ìš©\n",
    "X_train_pca = pca.fit_transform(X_train_orig)\n",
    "X_test_pca = pca.transform(X_test_orig)\n",
    "\n",
    "# PCA ì»´í¬ë„ŒíŠ¸ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€\n",
    "for i in range(n_components):\n",
    "    X_train_fe[f'pca_{i}'] = X_train_pca[:, i]\n",
    "    X_test_fe[f'pca_{i}'] = X_test_pca[:, i]\n",
    "\n",
    "print(f\"\\nPCA variance explained: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "print(f\"Final feature count: {X_train_fe.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”¼ì²˜ ì¤‘ìš”ë„ ê¸°ë°˜ ì„ íƒ\n",
    "# LightGBMìœ¼ë¡œ í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚°\n",
    "lgb_selector = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=-1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgb_selector.fit(X_train_fe, y_train)\n",
    "\n",
    "# í”¼ì²˜ ì¤‘ìš”ë„\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train_fe.columns,\n",
    "    'importance': lgb_selector.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# ìƒìœ„ í”¼ì²˜ ì„ íƒ (ì¤‘ìš”ë„ > 0ì¸ í”¼ì²˜ë“¤)\n",
    "selected_features = feature_importance[feature_importance['importance'] > 0]['feature'].tolist()\n",
    "\n",
    "print(f\"Selected features: {len(selected_features)} / {len(X_train_fe.columns)}\")\n",
    "print(f\"\\nTop 20 features:\")\n",
    "display(feature_importance.head(20))\n",
    "\n",
    "# ì„ íƒëœ í”¼ì²˜ë§Œ ì‚¬ìš©\n",
    "X_train_selected = X_train_fe[selected_features]\n",
    "X_test_selected = X_test_fe[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation ë¶„í• \n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_selected, y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§\n",
    "scaler = RobustScaler()  # ì´ìƒì¹˜ì— ê°•í•œ ìŠ¤ì¼€ì¼ëŸ¬\n",
    "X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "print(f\"Train shape: {X_tr_scaled.shape}\")\n",
    "print(f\"Validation shape: {X_val_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgb(trial):\n",
    "    \"\"\"LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1,\n",
    "        'objective': 'multiclass',\n",
    "        'metric': 'multi_logloss',\n",
    "        'class_weight': 'balanced'\n",
    "    }\n",
    "    \n",
    "    model = LGBMClassifier(**params)\n",
    "    \n",
    "    # 3-fold CV\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = cross_val_score(\n",
    "        model, X_tr_scaled, y_tr,\n",
    "        cv=skf,\n",
    "        scoring=make_scorer(f1_score, average='macro'),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Optuna ìŠ¤í„°ë”” ìƒì„± ë° ìµœì í™”\n",
    "print(\"LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...\")\n",
    "study_lgb = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=RANDOM_STATE)\n",
    ")\n",
    "\n",
    "study_lgb.optimize(\n",
    "    objective_lgb,\n",
    "    n_trials=30,  # ì‹œê°„ ì ˆì•½ì„ ìœ„í•´ 30ë²ˆë§Œ\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBest LightGBM score: {study_lgb.best_value:.4f}\")\n",
    "print(\"Best parameters:\")\n",
    "print(study_lgb.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb(trial):\n",
    "    \"\"\"XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 10.0, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'objective': 'multi:softprob',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'use_label_encoder': False\n",
    "    }\n",
    "    \n",
    "    model = XGBClassifier(**params)\n",
    "    \n",
    "    # 3-fold CV\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = cross_val_score(\n",
    "        model, X_tr_scaled, y_tr,\n",
    "        cv=skf,\n",
    "        scoring=make_scorer(f1_score, average='macro'),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# XGBoost ìµœì í™”\n",
    "print(\"\\nXGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...\")\n",
    "study_xgb = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=RANDOM_STATE)\n",
    ")\n",
    "\n",
    "study_xgb.optimize(\n",
    "    objective_xgb,\n",
    "    n_trials=30,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBest XGBoost score: {study_xgb.best_value:.4f}\")\n",
    "print(\"Best parameters:\")\n",
    "print(study_xgb.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ìµœì í™”ëœ ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„±\n",
    "best_lgb = LGBMClassifier(\n",
    "    **study_lgb.best_params,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1,\n",
    "    objective='multiclass',\n",
    "    metric='multi_logloss',\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "best_xgb = XGBClassifier(\n",
    "    **study_xgb.best_params,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    objective='multi:softprob',\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# CatBoost (ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹)\n",
    "best_cat = CatBoostClassifier(\n",
    "    iterations=800,\n",
    "    depth=10,\n",
    "    learning_rate=0.05,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=False,\n",
    "    auto_class_weights='Balanced',\n",
    "    task_type='CPU',\n",
    "    l2_leaf_reg=5\n",
    ")\n",
    "\n",
    "# ê°œë³„ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
    "models = {\n",
    "    'LightGBM': best_lgb,\n",
    "    'XGBoost': best_xgb,\n",
    "    'CatBoost': best_cat\n",
    "}\n",
    "\n",
    "model_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_tr_scaled, y_tr)\n",
    "    \n",
    "    # Validation í‰ê°€\n",
    "    y_pred = model.predict(X_val_scaled)\n",
    "    score = f1_score(y_val, y_pred, average='macro')\n",
    "    model_scores[name] = score\n",
    "    \n",
    "    print(f\"{name} Validation Macro F1: {score:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Model Performance Summary:\")\n",
    "for name, score in model_scores.items():\n",
    "    print(f\"{name}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì•™ìƒë¸” ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Voting Ensemble\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lgb', best_lgb),\n",
    "        ('xgb', best_xgb),\n",
    "        ('cat', best_cat)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Voting Ensemble...\")\n",
    "voting_clf.fit(X_tr_scaled, y_tr)\n",
    "\n",
    "# í‰ê°€\n",
    "y_pred_voting = voting_clf.predict(X_val_scaled)\n",
    "voting_score = f1_score(y_val, y_pred_voting, average='macro')\n",
    "\n",
    "print(f\"\\nVoting Ensemble Validation Macro F1: {voting_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Ensemble\n",
    "base_models = [\n",
    "    ('lgb', best_lgb),\n",
    "    ('xgb', best_xgb),\n",
    "    ('cat', best_cat)\n",
    "]\n",
    "\n",
    "# Meta learner\n",
    "meta_learner = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=3,  # Cross-validation for generating meta features\n",
    "    n_jobs=-1,\n",
    "    passthrough=False  # Don't include original features\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Stacking Ensemble...\")\n",
    "stacking_clf.fit(X_tr_scaled, y_tr)\n",
    "\n",
    "# í‰ê°€\n",
    "y_pred_stacking = stacking_clf.predict(X_val_scaled)\n",
    "stacking_score = f1_score(y_val, y_pred_stacking, average='macro')\n",
    "\n",
    "print(f\"\\nStacking Ensemble Validation Macro F1: {stacking_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ìµœì¢… ëª¨ë¸ ì„ íƒ ë° ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë“  ëª¨ë¸ ì ìˆ˜ ë¹„êµ\n",
    "all_scores = model_scores.copy()\n",
    "all_scores['Voting Ensemble'] = voting_score\n",
    "all_scores['Stacking Ensemble'] = stacking_score\n",
    "\n",
    "# ìµœê³  ëª¨ë¸ ì„ íƒ\n",
    "best_model_name = max(all_scores, key=all_scores.get)\n",
    "best_score = all_scores[best_model_name]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Model Selection\")\n",
    "print(\"=\"*60)\n",
    "for name, score in sorted(all_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{name}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ† Best Model: {best_model_name}\")\n",
    "print(f\"   Validation Macro F1: {best_score:.4f}\")\n",
    "\n",
    "if best_score >= 0.9:\n",
    "    print(\"\\nâœ… ëª©í‘œ ë‹¬ì„±! (Macro F1 >= 0.9)\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ ëª©í‘œì— ê·¼ì ‘ (í˜„ì¬: {best_score:.4f}, ëª©í‘œ: 0.9)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ëª¨ë¸ë¡œ ì „ì²´ í•™ìŠµ ë°ì´í„°ë¡œ ì¬í•™ìŠµ\n",
    "if best_model_name == 'Voting Ensemble':\n",
    "    final_model = voting_clf.__class__(estimators=voting_clf.estimators, voting='soft', n_jobs=-1)\n",
    "elif best_model_name == 'Stacking Ensemble':\n",
    "    final_model = stacking_clf\n",
    "else:\n",
    "    final_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nTraining final {best_model_name} on full training data...\")\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
    "test_predictions = final_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nì˜ˆì¸¡ ì™„ë£Œ!\")\n",
    "print(\"\\nì˜ˆì¸¡ ë¶„í¬:\")\n",
    "pred_distribution = pd.Series(test_predictions).value_counts().sort_index()\n",
    "for cls, count in pred_distribution.items():\n",
    "    print(f\"Class {cls}: {count:6d} ({count/len(test_predictions)*100:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "submission['target'] = test_predictions\n",
    "\n",
    "# íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "submission_filename = f'../submissions/advanced_{best_model_name.lower().replace(\" \", \"_\")}_f1_{best_score:.4f}_{timestamp}.csv'\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: {submission_filename}\")\n",
    "print(\"\\nì œì¶œ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "display(submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ì¶”ê°€ ê°œì„  ë°©ì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š ì„±ëŠ¥ ë¶„ì„ ë° ì¶”ê°€ ê°œì„  ë°©ì•ˆ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if best_score >= 0.9:\n",
    "    print(\"\"\"\n",
    "âœ… ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„±!\n",
    "\n",
    "ì¶”ê°€ ê°œì„  ì•„ì´ë””ì–´:\n",
    "1. **Pseudo Labeling**: Test ë°ì´í„°ì˜ ê³ ì‹ ë¢°ë„ ì˜ˆì¸¡ì„ í•™ìŠµì— í™œìš©\n",
    "2. **ë” ë§ì€ ì•™ìƒë¸”**: Neural Network ë“± ë‹¤ë¥¸ ëª¨ë¸ ì¶”ê°€\n",
    "3. **Post-processing**: Threshold ìµœì í™”\n",
    "4. **Test Time Augmentation**: ì—¬ëŸ¬ ë³€í˜• ë°ì´í„°ë¡œ ì˜ˆì¸¡ í›„ í‰ê· \n",
    "5. **ë” ê¹Šì€ Feature Engineering**: \n",
    "   - Frequency domain features (FFT)\n",
    "   - Clustering features\n",
    "   - Target encoding\n",
    "    \"\"\")\n",
    "else:\n",
    "    gap = 0.9 - best_score\n",
    "    print(f\"\"\"\n",
    "í˜„ì¬ ì„±ëŠ¥: {best_score:.4f}\n",
    "ëª©í‘œê¹Œì§€ ì°¨ì´: {gap:.4f}\n",
    "\n",
    "ê°œì„  ì „ëµ:\n",
    "1. **ë” ë§ì€ Optuna trials**: í˜„ì¬ 30 â†’ 100+\n",
    "2. **Neural Network ì¶”ê°€**: TabNet, Deep Neural Network\n",
    "3. **í´ë˜ìŠ¤ë³„ ìµœì í™”**: ì„±ëŠ¥ì´ ë‚®ì€ í´ë˜ìŠ¤ ì§‘ì¤‘ ê°œì„ \n",
    "4. **ë°ì´í„° ì¦ê°•**: SMOTE, ADASYN ë“±\n",
    "5. **ë” ë³µì¡í•œ ì•™ìƒë¸”**: Multi-level stacking\n",
    "6. **Feature Selection ì¬ê²€í† **: ë‹¤ë¥¸ ë°©ë²• ì‹œë„\n",
    "    \"\"\")\n",
    "\n",
    "print(f\"\\ní˜„ì¬ ìµœì¢… ì„±ëŠ¥: {best_score:.4f} ({best_model_name})\")\n",
    "print(\"\\në‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œ ì¶”ê°€ ì‹¤í—˜ì„ ì§„í–‰í•˜ì„¸ìš”.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}